	Содержание:	
1. Введение. Процессы / потоки
2. Примитивы синхронизции
3. Механизмы завершения потоков
4. Алгоритмы синхронизации. TLS
5. Ошибки || программирования
6. Snapshot атомарных регистров
7. Модели памяти и проблемы видимости
8. Профилирование приложений
9. Очередь Michael-Scott. Flat-combining
10. Транзакционная память
11. RCU
12. OpenMP и Intel TBB
13. Асинхронный ввод/вывод

	Lecture 1

multithreding  		|    	multiprocessing (isolated, safety,
			|	fault tolerance [segf])
			|	// real-time OS (QNX)
----------------------------------------------------------------------
+ share same addr space |    	shmem (see /dev/shm, used by browsers)
--> TLB is more 	|    	pipes (thread safety from box)
performance	 	|    	file
+ simply implement	|    	socket
+ more crossplatform 	|    	mmap
		 	|    	signals (async)

	TLB (translation lookeside buffer) - buffer for caching indexes 
after translation pages from virtual memory to physical memory
// Get page from cache ~60 times faster then calculate throw tables

	SSE (Streming SIMD Extentions or Vectorization) - extention
for CPU (for work with floating points)
// already exist mmx (multi-media extention)

__asm { // cpecific code ... };

	Degree of connectivity:
1) friend relation (C++ only)
2) inheriance
3) composition // class A { B instance_; }; don't exist separately
4) agregation // class A { B * ptr_;  }; exist separately

	Lecture 2 (primitives of synchronize)

In LP don't exist connection between primitives and data's
Linux ~ 16 millions code line's
~500 syscalls in Linux (POSIX standart)
Asm instructions ~ ns (because gigahertz in CPU ~ ns)
Call syscall dt ~= 1ms

	RAM
---------------------------------------------------------
| th_0	| th_1	|  ... 	| th_n	| kernel		|
|	|	|	| th_n	| (already in mem)	|
-------------------------------------^-------------------
	   | syscall		     |
	   ---------------------------
every syscall = swith context (bad performance)

mtx.lock() -> (wait if already exist thread in critical section)
	// citical section ...
mtc.unlock()

	Types of synchronise primitives
1) recursive
2) don't recursive

	Spurious wake up
specially added in Linux kernel, already add inf cicle for
condition wariables

	Named mutex - mutex with some name for synchronise in others
PROCESS (not threads)
// implemented in kernel only
// correct unlock in case crash of application

	Lecture 3 (part 1)

main (default ending)
 |
pthread_create()------->|
 |			|
 |			|
 |			|
 |			|
pthread_join()<---------|
 |
return 0

	Pthread API (oriented in Linux kernel)

1) pthread_cancel() - set flag in 'pthread' structure which means, 
that somebody wants to end thread
2) pthread_test_cancel() - check flag, if (true) 
	- call cancellation_points
	- call pthread_exit()
3) pthread_clenup_push()/pop() - add/remove cancellation point to stack

IMPORTANT: any thread can call pthread_cancel()
Many syscallc - cancellation points {wait, read, write, sleep} ~ 100 cnt

4) pthread_join() - is't cancellation point 
5) pthread_set_attr()
	- PTHREAD_CANCEL_ENABLE/DISABLE - turn on/off pthread_test_cancel()
// after set ENABLE - start auto check by pthread_test_cancel()
6) pthread_wait()
7) pthread_kill(id, signal) - send signal to certain thread 

thread 1		thread 2
main
 |
pthread_create()--------|
 |			|
 |			throw exeption;
 |
pthread_join()

NOTE: Cannot use try in thread1 to catch exeption from thread2
* exeption_ptr - can get first exeption in chain of exeptions

sockerts
 |
pcap - wireshark
     - tcpdump

    Lecture 3 (part 2)

Resourse - absolutly any: file, variable, etc...
Parallel programming - programming between lines

-----------------
| 32bit | 32bit | <- 64 buit number in 32 bit arcitecture
-----------------
    Data race situation:
move // part 1
                <-- interrupt
move // part 2

	Primitives os syncronization

Thread-save data structure create logical connection primitive with data
// in purpose don't think about race-condition {concurrent vector, ...}

IMPORTATNT:  
	- it don't assosiated with data (primitive---x---data)
	- take mtx == foll to kernel space
	- if not-recursive to lock second time == deadlock

List of primitives:
    - mtx (mutual exclusion) - resource of OS {recursive, not recursive}
    In current time (in Boost) exist more functionality primitives like as:
    - timed_mtx - lock resource by specified time value
    - shared_mtx - r/w lock (readers more than writers)
    - queue_mtx - K-limited wait
    - spin_mtx - when we often use lock/unlock for simple structure
    		 NOTE: it must't use with any system call
    

* thread save resource = primitive + resource
    |-> for example: exist "vector" and "concarrent vector"

* Deadlock: situation, when more tan 2 threads loops in infinitive cicle
    
* pthread mutex_t - structure of kernel space
    
* Exist resursive and not recursive primitives
    |                 |
    lock = 1 times    can lock > 1 times in one thread
                      |-> for example: concarrent vector
                                       |             |
                                       push_back     size
                      // and push_bask ans size use primitive
                      // buck_back call size --> deadlock if 
                      // not recursieve primitive
 
NOTES FOR EXAMPLE: can don't use recursieve primitive and separate safe 
and not safe methods. Better don't use recursieve primitives.

NOTE FOR PTHREAD MUTEX: is recursieve (more havily than simple mtx)

* CAS operations - compare and set // assemblym instruction
  |-> atomic operation on hardware lvl (cmpxchg +i486 was added)
// set new value only if 'old value == current value'

IMPORTANT: 
* Atomic variables - wrapper over primitives types with atomic
instructions over them. (implemented by just types and CAS operations)

* Ptimitives in C++ must be implemented by RAII
	Emample:
{   <-- scope
    lock();
    
    try {   // if exeption, then auto-unlock will be done by destructor
    }
    catch(...) {
    }
    ... <-- auto-unlock by destructor
}

// честные примитивы - те, что используют только ядро ОС
Early, mutexes was only in kernel space, then it have done partially in 
user space.

* Futex - fast userspace muTexes
- was added since Linux ~2.6 for optimizations
- based on CAS operations, but with kernel space supportion
- all modern primitives based on Futexes
- if exist case, when we can't go to kernel - so we do as well

* lll (low lvl lock)

	Lecture 4 (part 1)

Other primitive synchronize: "condition variables" 
// need for communication between threads
* For example: it solve such task as: "Produsers ans Consumers"

* If exist only two operations: w/r, then we don't need to lock the
data. Because no have difference. Data will'not be safer whatever.
NOTE: for integers with bytes < CPU bit depth => w/r is atomic

	increment in asm: (don't atomic)
1) move - read from RAM
2) inc  - register++
3) move - write back

	Implementation:
pthread_cond_wait(cond, mtx) {
	unlock(mtx) | atomic operation
	// sleep    |
		<-- signal (broadcast)
	// wake up  | WARNING: spurious wake ups	
	lock()
}

NOTE: See example with barrier (other primitive) on lection
// Need for simultaneous starting N threads or other specific logic

* Methods for work with condition variable:
    1) pthread_cond_broadcast   => get up + try lock all threads
WARNING: can call without lock mtx! In Java (for example) it forbidden.
But dinamic analizer (as Valgring) throw warning in this case
    2) pthread_cond_signal      => get up + try lock one thread
    3) pthread_cond_wait        => unlock + sleep, then lock again

* Sleep/Get up in Kernel implemented by signals but it hidden 
from us
WARNING: Exist such bad situations: spurious wakeups!

	ALGORITHMS SYNCHRONIZE FOR STRUCTURE "FORWARD LIST":
Only operations: {delete, insert, find} 
// implementation of set 'abstract structure'

#1) brute synchronize (very slow)
	EXAMPLE:
lock() for all structure
---   ---   ---   ---
| |-->| |-->| |-->| |-->null (forvard list)
---   --    --    ---
unlock() for all structure

#2) Shared_mtx

#3) Slightly synchronize 
// ttep by slep lock/unlock each node (too much memory)

EXAMPLE:        time1
lk/   lk/
ulk + ulk
---   ---   ---   ---
| |-->| |-->| |-->| |-->null (forvard list)
---   --    --    ---
                time 2
      lk/   lk/
      ulk + ulk
---   ---   ---   ---
| |-->| |-->| |-->| |-->null (forvard list)
---   --    --    ---
		time 3
etc...

Summaru:
* 2 locks need (on (k-1)-th and k-th) for deleting k-th element
* 1 lock (on k-1-th) for inserting (but 2 locks for go to the position)
* 2 locks for finding element (2 locks except insert operation)

#4) Copy LIST for each thread and work WITH COPY in thread
// this case ose in over highload systems (as Databases or destrubuted storage system)

	Too hard algs... (will't be in course)
    ---   ---   ---   ---
th1 | |-->| |-->| |-->| |-->null (copy for th1 from master)
    ---   ---   ---   ---
     ^     ^     ^     ^         
     |     |     |     |    pointers for synchronize result
    ---   ---   ---   ---
th2 | |-->| |-->| |-->| |-->null (copy for th2 from master)
    ---   --    --    ---
     ^     ^     ^     ^ 
     |     |     |     |
    ---   ---   ---   ---
th3 | |-->| |-->| |-->| |-->null (copy for th3 from master)
    ---   --    --    ---

	Lecture 4 (part 2, algorighms sychronizations)

#1) Оптимистичная синхронизация
    * в отличие от тонкой синхронизации лочим только 
      те элементы которые нужны
    ПРОВЕРКИ:
    - проверяем что элементы по-прежнему в списке
      проходя ещё раз по списку
    - проверяем что эл-ты по-прежнему соседние
    ИТОГО:
    - если хотя бы 1 условие не выполняется - повторяем
      алгоритм заново

NOTE: позволяет улучшить производительность системы в целом
а не отдельных потоков
ПРЕИМУЩЕСТВО: потоки друг с другом не "сталкиваются", но 
требуем больше процессорного времени (жертвуем процессорным временем)

NOTE: удаление корректно обрабатывается на языках со сборщиком
мусора

----    -----
|  | -->|   | // в этом случае все равно получится корректно работает
----    ----- // но ссылку у удаленного эл-та нужно занулять
----  _/
|  |-/    удаленный элемент
----
При таком подходе большее число потоков могут || получать доступ
к структуре -> общая производительность системы растет


#2) Ленивая синхронизация
* На чтение данные используются намного чаще. Значит
оптимизируем операцию поиска.

* Помечаем просто "галочкой" что элемент был удален и оставляем
его висеть в памяти. (при снижении пиковой нагрузки удаляем)

* Больше не требуется проходить по списки ещё один раз. Достаточно
проверить 2 флага и что элементы соседние.

* Поиск производится всегда по "ключу"

* Для адресации в 64-битной архитектуре в адресе используется 48 бит

NOTE: поиск при таком подходе wait-free. Т.е. мы гарантированно за O(n)
выдаем результат (точнее за 1 проход)


#3) неблокирующие алгоритмы (lock-free)

----    -----
|3 | -->|5  | 
----    -----
        /  // создаем ссылку на следующий эл-т
----  _/
|4 |-/   // наш новосозданный эл-т
----
NOTE: с помощью CAS стараемся так же установить ссылку с предыдущего
на текущий эл-т, итого получаем:

----    -----
|3 |    |5  | 
----    -----
 |      /
----  _/
|4 |-/  
----
	Некоторые выводы:
* отсутствуют примитивы синхронизации (но сущ CAS), это гарантирует что
ни одни потоки не влияют на другие потоки

* если в ЛЮБОЙ момент времени мы остановим К потоков из N, то оставшиеся
(N-K) потоков гарантированно доделают свою работу

* если даже CAS возвращает false то мы знаем, что другому потоку
удалось сделать свою задачу и система в целом прогрессирует

* см алгоритм снятия snapshop-ов атомарных регистров


#4) wait-free алгоритмы
Более жесткий класс алгоритмов, с большими ограничениями

Если мы остановим K потоков из N в произвольный момент времени, то 
оставшиеся (N - K) потоков не просто когда-то завершат свои задачи,
а сделают это не более чем за M загов, M != f(число потоков), 
зависит только от размера структуры данных.
// допустим CAS не сработает не более чем 5 раз у каждого

NOTE: обычно структуры не полностью lock-free или wait-free а лишь
отдельные операции над структурой
// Поиск wait-free в нашем примере, т.к. за N шагов он гарантированно
заканчивается


	Как использовать CAS со структурами > машинного слова?
Импользуем указатели на эти структуры (приходится полагаться, что
структура иммутабельная)

	Снятие snapshot-ов атомарных регистров
SWMR (single wrine multiple readers) - важно только для wait-free,
у нас lock-free алгоритм, нам это ограничение не важно

 th1     th2      // only write in yourself registers
------  ------    Lock-free snapshots
| 0  |  | 0  |		((0, 0), (0, 0))
| 0  |  | 0  |		
-----   ------
Инкрементируем версию при записи

 th1     th2   		((x1, v1), (x2, v2))
------  ------	        
| 2  |  | 0  |		((0, 0), (0, 0))
| 1  |  | 0  |		((2, 1), (0, 0))	
-----   ------

NOTE: если версии у i-го и (i+1)-го snapshot-а совпадают, то никто
кроме нас гарантированно ничего в регистр не писал

в printf внутри воддерживается атомарный вывод!

	Lectute 5 (Ошибки параллельного программирования)
1) * deadlock
valgring - VM for linux, as proxy any actions
	--tool=helgrind - for find deadlocks
	--tool=memcheck - default memory checker
	--tool=massif   - dray image memory=f(time)
	
*helgrind - анализируется журнал захвата и освобождения
примитивов синхронизации, если существует место с разным
порядком захвата - выдает ошибку

*ThreadSanitaizer - more faster, from Google

Эти средства должны понимать, что примитив это примитив.
С нашими собственными примитивами работать не будет.
На spinlocks работать так же не будет.

NOTE: Не забываем про deadlock с fork-ом и захваченным mtx-ом ;)
Т.к. будет всего 1 поток. 
WARNING: В speenlock нет проверки на TID, в отличие от mtx-a



Механизм отображения страниц в новом процессе - ленивый.
Т.е. страницы физически копируются только тогда, когда
в исходном процессе произошло изменение памяти.

POSIX говорит, что OS вправе отказать делать fork в
многопоточном приложении.

В ядре используются не mtx-ы а семафоры, а те в свою очередь
не принадлежат потоку



2) *race-condition - ищем теми же самыми програмными средствами
, но немного другими алгоритмами. (алгоритмически неразрешимая)

NOTE: На проявление race-condition влияет количество ядер CPU и компилятор



3) * инверсия приоритетов - чем больше мы захватили ресурсов, тем
выше у нас приоритет у пранировщика.
чтобы, если приоритет с низким приоритетов получал больше

NOTE: процессорного времени, чтобы скорее освободить ресурс и обратно
упасть в нишу с низким приоритетом


4) * ABA проблема - современные аллокаторы имеют оптимизации и часто
на одно и тоже физическое место может вставиться другой объект

часто полагаемся что разные адреса == разные данные, но в данном случае
эта логика ломается

является большой проблеммой для нативных языков, как C/C++, в которых
нет сборщика мусора

используем свободные 16 бит для реализации счетчика, при каждом добавлении
делаем инкремент, тогда CAS операции проходить не будут

на ARM существуют LL/SC операции

	Лекция 6 (Snapshot атомарных регистров)
Все структуры считаем иммутабельными. Изменение = копирование + 
изменение + подмена указателя (посредством CAS операций)

	1) Lock-free алгоритм
		иммутабельная струкутра
--------       ---------
| ptr  |------>| value |
--------       |-------|
	       |version| <-- изменяется писателем
	       ---------

Считываем по 2 раза подряд до тех пор, пока версии не будут отличаться.
Сравнимаем вектора версий для каждого регистра отдельно. Т.е. версии
первого регистра с первым регистром, второго со вторым и т.д.

Snapshot олицетворяет состояние системы. И нам важно штобы мы получили
действительно существовавшее значение в системе, чтобы принять верное
решение

vector указателей на структуры
---------
| ptr0  | --> struct 0
---------
| ptr1  | --> struct 1
---------
| ptr2  | --> struct 2
---------
| ptr3  | --> struct 3
---------

Подменяем указатели в векторе, благодаря CAS-опрерациям (т.е. атомарно)

	2) Wait-free алгоритм
В lock-free алгоритме, потоку, который делает снимки может очень
часто невезти

ОГРАНИЧЕНИЕ:
SWMRR (single writer, multiple readers register), т.е. в каждый поток
может писать не более чем один прибитый к регистру поток

Платим памятью и средней производительностью в угоду гарантий
Используются в ответственных системах реального премени

	Лекция 7 (Модели памяти и проблемы видимости)

	1) Поиск ошибки с memory reordering:
*strace -p
	по аналогии с gdb подключаемся к сущ процессу
	еще один полезный способ анализа приложения

VFS (virtual file system) - дополнение к основной FS, не
существует реально на жестких дисках
	/dev - виртуальные файлы для устройств
	/sys
	/proc - вирт файлы для процессов (считываем не влияя
	на процесс, подглядываем)
	
Можем посмотреть стек потока и отследить сколько времени поток провел
на CPU и сделать вывод - стагнирует он или нет

	2) Общие сведения
Cache line in x86-64 = 64 bytes, регистры работают с кешом
а не памятью напрямую

В кеш не может подгрузиться из памяти не 64 байта, т.е. если
мы запросили 8B то подгрузятся все 64

Если в каком-то ядре уже в кеш-линии существуют эти данные,
то мы не полезем в память, а считаем из кеша соседнего ядра

Для таких случаев существуют протоколы поддержки когерентности
(согласованности) кешей процессоров

	
	3) Учебный протокол работы ядер с кешами
Рассматриваем учебный протокол MESI, где каждая буква аббревиатуры
описываем состояние кеш-линии

LRU (list resently used) - выдавливаем из кеша последнюю наименее 
используемую кеш-линию в память (так происходит синхронизация кеш-линии
и памяти)

Брокер - посредник между памятью и кешом, который отвечает на запросы процессора
на конкретную память и высылает инструкции по решению данного запроса (читать из
памяти или из кеша другого CPU)

Состояние кеша процессора хранится в каждой линии кеша процессора (+ 2 bit)
	*I (invalidated) - кеш линия не используется
	*E (exclusive) - монопольное владение учатком памяти (нет у других ядер)
	не нужно ничего ни с кем согласовывать
	*S (shared) - когда > 1 кеш-линии в разных CPU используют одну и туже
	ячейку памяти (брокер не говорит читать данные из раза в раз из памяти, 
	а говорит считать из кеш линии другого CPU)
	*M (modified) - линейка кеша переходит в это состояние, когда он находился
	в состоянии (E) и произошли изменения

Если мы находимся в состонии (S) и сделали изменение в кеш-линии, то мы посылаем 
брокеру сообщение о том, что другие ядра должны в нами синхронизироваться. После чего
дожидаемся подтверждения от ВСЕХ ЯДЕР и только после этого продолжаем свою работу.
Получаем своего рода блокировку.

Кеш нужен только для оптимизации, но при этом он прозрачен с точки срения логики.
Т.е. мы работаем, думая будто все процессоры обращаются в оперативную память каждый раз.


	4) Дополнение из реального мира к учебному примеру
В реальном мире добалено ещё 2 буффера для того, чтобы не дожидаться подтверждения
от всех ядер а продолжать дальше работать, это store buffer[1] and invalidate queue[2]

В store buffer мы скидываем кеш линию после её модификации в состоянии (S), а в 
invalidate queue помещается запрос о том, что данную кеш линию нужно синхронизовать,
чтобы процессор делал это когда ему удобно и не сбивал свой налаженный конвеер

Подтвержение приходит тогда, когда запрос попадает в очередь, а не когда данная команда
выполняется непосредственно на процессоре, т.е. все ядра обманывают нас :(


	5) Какие ошибки можем получить в соответствии с данной архитектурой
	* пример с memory reordering *
Данный memory reordering происходит не из-за компилятора (некоторых оптимизаций)
, когда он переставляем местами инструкции, а зависит только от реализации 
архитектруры. Т.е. на этапе исполнения железо сочло нужным действовать так.

Пример с запоздалой обработкой запроса (read/invalidate) во втором процессе
Изначально все 2 переменный разшарены между th1 and th2 и инициализированы нулями.

 	th1		|	th2
------------------------|-----------------------------
f() {			|	g() {
	int a = 1;	|		while(b == 0)
	int b = 1;	|		assert(a == 1)
}			|	}
------------------------------------------------------

Может случиться такая ситуация, когда переменная 'a' находится
в состоянии (S), после модификации в th1 отправился запрос (read/invalidate)
для th2 (поместился в invalidate queue), но слишком поздно из этой очереди
обработался и у нас сфейлился assert

Процессор не знает что в данном случае переменные 'a' и 'b' связаны между собой
логически.

ВАЖНОЕ ЗАМЕЧАНИЕ:
Если бы переменная 'a' не была в состоянии (S), то th2 вынужден был бы ждать,
чтобы получить эту переменную и такой ситуации не возникло.

Или если бы запрос инвалидации отработал до 'assert', то такой ситуации так же
не было бы.

ВЫГОДА ИЗ ЭТОЙ АРХИТЕКТУРЫ:
В данном случае меняем согласованность на производительность, т.к. мы обрабатываем
запрос инвалидации когда нам удобно и не прерываем текущий сформированный конвеер 
комманд на несколько тактов вперед

Напоминает CAP теорему, но в нашем случае доступность преобладает над 
консистентностью

Если хотим максимальную производительность, то делаем так, чтобы память не 
пересекалась, тогда и кеши не будут пересекаться

	Схема данной более 'реальной' архитектуры
read/invalidate отправляется брокеру после обновления
расшаренной кеш-линии

CPU0			CPU1
---------  read/	---------	
|line01	|  invalidate	|line11	|
|line02	|-->  брокер    |line12	|
---------    --------	---------
   |	     |	    |	   |
store-buffer |      |	store-buffer
---------    --------	---------
|	|     |	   |	|	|
|line02	| <---|	   |	|	|
--------- ожидание |	---------
   |	  подтв.   |	   |
invalidate	   |	invalidate
queue		   |	queue
---------	   |	---------
|	|	   |-->	|line02	|
|	|    добавление	| 	|
---------    значений   ---------
             на инвалидацию

---------------------------------
|	RAM			|
|				|
---------------------------------

	6) Попытки все это детерминировать (memory bariers):
Существуют ассемблерные инстуркции которые позволяют:
	- реально дождаться подтверждения об запросе на инвалидацию
	  от других CPU
	- прервать текущий конвеер и честно выполнить все инвалидации
	  из очереди инвалидаций

В store buffer находятся все запросы, которые ожидают подтверждения на 
инвалидацию

	WRITE MEMORY BARIER:
smp_wmb() - ожидает опустошения store-buffer (т.е. дожидается всех подтверждений
на инвалидацию)
Работает дольше rmb, т.к. ожидаем подтвержение от всех ядер

	READ MEMORY BARIER:
smp_rmb() - принудительно выполняет все запросы на инвалидацию находящиеся в
invalidate queue
Работает быстрее wmb, т.к. выполняет свою очередь и все

smp_mb() - вызывает обе функции

	Существует 4 вида теоретических барьеров памяти
Впоследствии транслируются в ассемблерные комманды на 
конкретной архитектуре

---------------------------------
|load	1	|load	2	|
|load		|store		|
---------------------------------
|store	3	|store	4	|
|load		|stre		|
---------------------------------

Все барьеры влияют на то в каком порядке мы реально увидим эти 
значения переменных

4 - все ЗАПИСИ до барьера, произойдут гарантированно раньше
всех записей после барьера

store1|
store2| 
...   | записи до берьера
storeN| (произойдут ранее)
------------------
StoreStore barier| ждем опустошения store-buffer-а
------------------
store1'|
store2'| записи после барьера
...    | (произойдут позднее)
storeK'|
--------

Барьеры как примитивы синхронизации - должны работать согласованно
во всех местах. Т.е. если я ставлю барьер в одном месте, то я должен
поставить барьер и в другом месте

	7) Модели памяти:
Существуют некоторые платформы, которые гарантируют, что некоторые
виды барьеров нам ставить не нужно. Т.e. у некоторых процессоров
может отсутствовать invalidate queue (т.е. все переменные сразу, по честному
инвалидируются) или нет store-buffer-а (т.е. сразу по честному ждем всех
подтверждений)

1, 2 - логически объединили в acquire (операции чтения до любых операций в секции)
2, 4 - логически объединили в release (операции записи  после любых операций в секции)

acquire - release semantics - некоторого рода логический захват и освобождение ресурса.
Данная пара не позволит никакому коду выйти за пределы секции между этими 2 барьерами.

	* sequential-consistensy 
		- применяются все барьеры после каждой операции
	* strong-ordered 
		- все, кроме store-load. Бесплатная
		- acqure-release семантика при каждом чтении и каждой записи (x86)
	* weak-ordered 
		- нужно применять все барьеры самостоятельно (ARM)
	* super-weak 
		- чисто академический

Интересующие нас модели памяти: strong-ordered and weak-ordered

В С++ ключевое слово volatile никак не относится к барьерам памяти (в отличии от java).

В С++11 появились atomics и модели памяти в них появились в том числе. Когда используем
атомики по умолчанию применяются все барьеры памяти (будто работаем с моделью памяти:
sequential-consistensy). Но можно явно указывать какие барьеры памяти мы хотим применять.

В С++ существуют только следующие модели для атомиков:
	- seq-cons
	- acqure
	- release
	- relaxes

	Лекция 8 (Профилирование)

1) *valgrind
	--tool=callgrind - строит граф вызовов
Чтобы посмотреть созданный файл, используем программу:
	kcachegrind <filename>
Не может профилировать по времени исполнения, но профилирует
по количеству выполненных инструкций (из-за архитектурных
особенностей)

Valgrind в многопоточном приложении (исходя из его архитектуры)
сделает так все потоки выстроятся в один (физически будет исполняться
на 1 ядре), поэтому время профилирования может увеличиться

Лучше не использовать в больших и реальных приложениях, т.к. работает
медленнее ~[60-120] раз

2) *time
	read - время в реальной жизни
	user - суммарное время по всем ядрам на процессорах
	sys  - процессорное время но в kernelspace
	(время во сне не считается, т.к. программа не находится
	на процессоре)

3) *VTute - профайлер для х86 процессоров от Intel
Дотаточно старая и сложная программа. Архитектурно зависимая но 
при этом комплексная система профилирования

* ldconfig - утилита, отображающая все динамические библиотеки для
динамического линковщика

* ldd - print shared object dependencies у конкретного исполняемого модуля.
Порядок определяется топологической сортировкой по всем зависимостям

Профилировщики типа VTute работают через механизм LD_PRELOAD, где
берутся и подменяются некоторые функции, в том числе и 
системные, чтобы отслеживать поведение программ. По аналогии работают
антивирусы в Linux - их библиотека кидается в LD_PRELOAD, и таким образом
она находится раньше по списку выбора чем оригинальная

Когда работает динамический линковщик, наша фейковая одноименная
функция находится в библиотеке, которая раньше по списку (см комманду ldd)
и поэтому именно она подставляется в место вызова а не оригинальная функция

Так же используется PMU (Performance Monitoring Unit) это некоторый 
аппаратный профилировщик, который предоставляет API и  дает более 
низкоуровневую информацию по типу: 
	- cache hits/misses 
	- частоту
	- число выполненных инструкций

4) *gprof (GNU Profiler) - это инструмент анализа производительности 
для приложений Unix, требует пересобирать приложение с ним

	Лекция 9 (Очередь Michael-Scott. Flat-combining)

Линеаризуемый алгоритм - тот который выполняется корректно при
любых расстановках точер линеаризуемости в разных потоках

Пример с вызовом функции из двух потоков в один момент времени:

call f1 in th1
|---------------|
   ^
   |
точка линеариз в th1


call f1 in th2
|---------------|
	^
	|
точка линеариз в th2

------------------------------------> time

Если алгоритм корректен, то он не зависит от относительного порядка
точек линеаризуемости. Т.е. таких критических точек для многопоточных
программ (например: обращение к критической секции)

Важное напоминание, что lock-free алгоритмы не должны никого ждать.
Если мы отключим несколько потоков в произвольный момент времени, то
оставшиеся должны окончить работу

	Очередь Michael-Scott
Вставку производим 2-мя CAS операциями
Удаление 1-й CAS операцией

Каждый поток помогает нам перетаскивать хвост, если тот указывает не на
последний элемент. 

Таким образом соблюдается инвариант, что:
хвост отстает от последнего элемента не более чем на 1 позицию, иначе бы
другой поток увидел бы это и помог нам перетащить указатель.

Мы не проверяем статус фикса хвоста в CAS операции, потому что нам это
безразлично. Если у нас не прошел CAS значит кто-то другой сделал это и 
при этом сделал именно этот CAS, т.к. все другие потоки сначала смотрят
на это корректность хвоста и фиксят его.

	
	Flat-combining
Специфичный подход к проектированию структур данных.
Адаптируем интерфейс потоко-опасной структуры к потоко
безопасному интерфейсу

Использует TLS для создание структуры для каждого потока.


			Flat combining
Никакого отношения к lock-free не имеет

	предоставляет	---------------------------------
	интерфейс ---->	|try_lock_global_mtx		|
		  |	|TLS1	TLS2	TLS3 .. TLSN	|
don't thread-safe |	|-------------------------------|
structure	  |	|	|	|	|	| действия для каждого
---------	  |	--------------------------------- потока (список публикаций)
|	|	  |	|	
|	|----------	|
---------		|
			|
			предоставляет интерфейс:
			
			* do (<enum_action>, [args...]) - публикация операции
			| Считаем атомарным действием и захватывать примитив
			нам для этого не требуется
			| Для каждого потока можно хранить лишь 1 публикацию, при
			попытке добавить вторую это станет блокирующей операцией

			* check (<операция>) - если некому выполнять операции
			то мы становимся рабочей силой и выполняем все операции
			для всех потоков (с захватом блокировки). 
			| Если кто-то уже за нас сделал это, то просто забираем 
		--------результат
		|
Если у нас не получилось стать комбайнером и при этом действие ещё никто
не выполнил, значит у кого-то получилось захватить примитив и этот кто-то
в процессе выполнения задач. В таком случае мы просто продолжаем работу 
и пробуем позже, когда нам это будет удобно

Возвращаемое значение после действия будет лежать в TLS, а не публикация 

Ожидать мы можем по любой стратегии: sleep, spinlock, leazy check, и т.д.

	ПЛЮСЫ:
1) Имея такой фреймворк, можем быстро оборачивать методы не потоко безопасных
структур.
2) Унифицированный интерфейс
3) Реализация специфичной логики в соответствии с парами операци, к примеру:
push + pop = ничего не делать
4) Повышается локальность кешей, т.к. только один поток работает со структурой
в 1 момент времени

Локальность кешей == меньше кеш-линий шарятся между другими процессорами
* Это значит что сильно уменьшается отработка протоков когерентности
кешей процессоров

См. benchmarks в статье к лекции

	Лекция 10 (Транзакционная память)

* Software transaction memory (STM) - прослойка между приложением и RAM
ЯП которые не имеют доступа к памяти (Java, c#)
- Haskell имеет STM как стандарт ЯП
- В реальной промышленности STM не применяется

* Hardware transaction memory (HTM) - сущ. поддержка со стороны
расширений CPU. Компилятор преобразует некоторые абстракции
в специальные низкоуровневые инструкции

Пропускная способность SkipList (см структуру) на транзакционной
памяти значительно превосходит обычную грубую синхронизацию почти
на порядок в числе операций

	ПРЕИМУЩЕСТВА
1) отсутствие блокировок
2) лучшая производительность <=> лучшая утилизация CPU
3) выше уровень абстракции - что защищаем а не как
4) возможность отката

	Как может быть реализована аппаратная поддержка:
Добавляем доп. флаг в состояние линий кеша процессора (находится
ли данная кеш-линиия в транзакции или нет) + специальные инструкции 
для установки и снятия этого флага

Если другой процессор попробует что-то сделать с этой памятью - то он 
подгрузит кеш-линию из другого процессора (или уже подгрузил). В таком случае
исходный процессор будет уведомлен о том, что кто-то попытался обратиться
к памяти и транзакция будет откачена.

Если у нас процессор Intel > 2012 года, то скорее всего есть данная аппаратная
поддержка (У AMD процессоров данного расширения пока что нет)
* TSX (Transactional Synchronization Extensions) - просто дополнительный
набор комманд (начать транзакцию, закончить, отменить и т.д.)

	Ограничения:
1) Структура должна вмещаться в кеш (1-го уровня ~[1-3] MiB), иначе за 1 
транзакцию невозможно обновить структуру
2) Архитектурные ограничения (поддержка)
3) Должны уложиться по времени в квант времени, выделенный планировщиком

См. графики вероятности отката от: 
	- размера структуры
	- числа операция над структурой

В современном GCC мы можем использовать TM начиная с GCC 6.1
(реализая в libitm.so (intel transaction memory))

Use flag: -fgnu-tm, используем код в секции 'synchronized { code... }'

Если не проверяем явно поддержку со стороны архитектуры: выкидывается исключение
'неизвестная инструкция'

Существуют статусы выполнения транзакций в соответсвии с которыми можно
обрабатывать результат

	Как сочетаются барьеры памяти и TM?
На момент окончания транзации процессор применет все изменения в 
invalidation queue и тем самым все корректно отработает

Но если есть логика, которая завязана на memory reordering то
атомики использовать придется
// тема сложная, сказать точно нельзя

В случае TM существует некоторый промежуточный слой в приложениях (примитивы)
который мы можем оптимизировать и это повлияет на большое число приложений
в мире

Существует 2 API for TM
	1) для встраивания в примитивы с fallback-ом на честные примитивы
	2) для настоящей работы как с транзакциями

	Лекция 11 (RCU)
Read Copy Update - специализированный примитив синхронизации
По стратистике структур данных которые читаются - больше

	(1) Рассмотрим RCU в kernelspace:
* Структура ссылочная и иммутабельная (добавление, удаление, поиск)
* Обеспечиваем быструю, независимую работу читателей
NOTE: remove - удаление логическое, а delete - физическое

Существуют 2 метода:
	- rcu_read_lock()
	- rcu_read_unlock()

	ГАРАНТИРОВАННОЕ ЧТЕНИЕ:
Если мы начали чтение в ядре какой либо структуры, то мы его закончим 
на том же вычислительном процессоре (Если не стоит флаг CONFIG_PREEMPT см.
/boot/config*-generic). Т.е. не произойдет никаких прерываний и нас 
не снимут с процесса благодаря этому флагу.

В таком случае имплементация мотодов абсолбтно пустая. Как такое 
реализовать (в kernelspace)?

1) Сначала перекидываем ссылку атомарно

----     ----     ----
|  | --> |  | --> |  |
----     ----     -^--
 |                 |
 -------------------
remove (логическое удаление)

2) forEach((вычислительное ядро) => { ничего не делаю})
т.е. занять ядро - гарантированно узнать, что читатель закончил
работу с этой структурой и эту структуру можно удалить физически (delete).

3) новые же операции чтения не получат удаленную структуру, т.к.
мы атомарно на п.1. перекинули ссылку.

ИТОГО: (с данным флагом) получаем нулевой overhead для читателей.
Сделали потокобезопасную структуру данных при том условии, что
читатели абсолютно ничего не делают.

	(2) Реализация в userspace с флагом
Если мы компилируем ядро с этим флагом, то подход немного меняется:
	1) читатель когда начинает операцию копируем себе некоторую глобальную
	переменную (просто число)
	2) писатель перед удалением инкрементирует это число и ждет, когла
	все читатели перейдут на новую эпоху
	3) новая эпоха у всех читателей в данный момент времени - гарантия того,
	что старую структуру данных никто не может использовать

Подобные структуры данных используются в ядре с 2003, без этой структуры была
бы значительная деградация в производительности.

	(3) Реализация в userspace
Схема работает, только если писателей в 1 момент времени <= 1, поэтому 
для них нужен mtx, чтобы не было конфликта изменения эпох + когда 
регистрируется новый писатель так же захватывается глобальный mtx

В userspace всегда есть механизм регистрации потока, в kernelspace такого нет,
т.к. мы знаем число ядер и можем по всем пройтись.

	(4) специфичный случай
В ядре должна быть обрпботка случая изменения числа ядер. Т.к. в некоторых
серверных решениях можно в процессе работы добавлять модули, воспринимаемые как
вычислительные ядра.

Флаг CONFIG_HOTPLUG_CPU позволяет добавлять ядра на лету.
Существует cтандартная реализация RCU в userspace, см. ldconfig -p

rcu примитив может быть так же рекурсивным по аналогии с рекурсивым mtx.

	Лекция 12 (OpenMB IntelTBB)
Пул потоков - шаблон проектирования для || выполнения задач.

	(1) Очередь задач
-----------------------------------------
| task1	| task2	| task3 | ...	| taskN |
|	| 	|	|	|	|
-----------------------------------------
   |	   |	    |_______ 	отображение задач		
   |       |		   |	на потоки, которые из выполняют
---------------------------------
| th1	| th2	| ...	| taskK |
|	| 	|	|	|
---------------------------------
   |	   |	    _______|
   |	   |	    |
-----------------------------------------
|future1|future2|future3| ...	|futureN|
|	| 	|	|	|	|
-----------------------------------------
Потоки (жиксированное число или изменяемое)
* Если задач нет для потока, то тот просто спит.
* Синхронизируем сон/работу через condition variables

Итого: имеем потокобезопасную очередь, горячие потоки и механизм
отображения задач/future object на потоки с оповещением этих потоков

В ответ на добавление задачи в очередь получаем объект, который
называется future. Это универсальный объект, во многих языках,
который позволяет отслеживать ход выполнения асинхронной задачи.

Future methods (интерфейс взаимодействия с потоком)
	- get - блокирующее ожидание результата
	- isFinish - проверка состояния
	- cancel - отмена потока

	(2) OpenMP (open multi processing)
Интерфейс для многопоточного программирования, развивается но слабо
Решение для Fortran, C/C++

* Области видимости переменных:
Существуют директивы препроцессора, определяющие поведение внешних
переменных по отношению к внутренним.
	- sharing (внешние переменные передаются по указателю)
	- private (внешние переменных никак не шарятся, а просто объявляются
	локально)
	- и т.д.

* Средства (директивы )синхронизации:
	- возможность объявлять критич. секции
	- явно объявлять атомарные операции

ИТОГО: xорошо использовать для простых вычислений, которых ложатся 
в вилочный параллелизм

	(3) IntelTBB (intel threading blocks)
Фреймворк || программирования для С++. Очень обширное количество
реализованных решений. Очень мощный инструмент.

Что имеется:
	- параллельные алгоритмы
	- многопоточные контейнеры
	- аллокаторы памяти
	- примитивы (лучше использовать std::, об этом
	говорит и сам Intel)
	- планировщики задач

Единицей планирования для TBB является 'задача' (TBB Task). При этом мы 
не имеем возможность создать несколько pool-ов потоков, т.к. все задачи 
проходят через главный планировщик, который скрыт от нас.

--------------------------------------
 потоки | очередь задач каждого потока
--------------------------------------
|	| ___    ___    ___   	|
| th1	| |_| -> |_| -> |_| ->	|
|	|			|
---------------------------------
|	| ___    ___    ___	|
| th2	| |_| -> |_| -> |_| ->	|
|...	|			|
---------------------------------
|	| ___			|
| thN	| |_| ->		|
|	|			|
---------------------------------

Где N обычно - это число ядер.
Если задачи у потока закончились, он их может украсть
у другого потока.

С tbb лучше будет работать сам Intel-овский компилятор.
Мы говорим что мы делаем, но не где. И это хорошая абстракция.

Так же появилась такая важная вещь, как 'дерево задач':
	- можно создавать задачи в виде объекта, добавлять
	дочерние задачи как рекурсивный вызов и ждать эти 
	'поддеревья задач'
	// при этом ожидание будет лишь с точки зрения
	выполнения кода построчно, но для потоков никакого
	ожидания не будет

Узким звеном после синхронизации (которую можно избежать), является
работа с памятью. Даже если мы напишем свой аллокатор, он все равно 
будет требовать некоторой координации для выделения памяти.

Решаем это резервированием некоторого количества памяти под каждый поток.
Аналог TLS. (Scalable аллокатор в TBB для || кода)

Есть также cache-align аллокатор. Просто добиваем размер аллоцированной
памяти до значения кратного кеш линии + выравниваем. Это поможет 
избежать ситуации приведенной ниже

	CPU0			CPU1
-----------------	-----------------
|cache lineK	|	| cache lineN	|
---------------------	---------------------
|  x	|  y	| S |	|  x	|  y	| S | <-- shared cache line
---------------------	--------------------- 	  (сост. кеш линии)
   ^				   ^
   |				   |
работаем с 			работаем с 
переменной x			переменной y

* Логически наша работа отлично распаралелена, но в действительности
ядра процессора конкурируют за значения к кеш линиях.
* Такая ситуация называется 'fase sharing'

В транзакционной памяти можно использовать подобный аллокатор, но
совсем не обязательно. Т.к. вряд ли такое являение будет часто влиять
на транзакции

Смена аллокатора вполне естественное явление для контейнера, которая 
не требудет дополнительных усилий. Т.е. не нужно закладиваться заранее
и можно решить отложенно.

	В TBB есть такой шаблон проектирования как pipeline.

---------  x	---------  f(x) ---------  g(f(x))
|	| ----->|	| ----->|	| ---------> и т.д.
---------	---------       ---------

Можно к примеру распараллелить функцию №2 в конвеере.
Так же существует модель графовых вычислений. Расширенный pipeline.

	Лекция 13 (Асинхронное i/o)
i/o: блокирующее, не блокирующее

Блокирующее чтение сокета не подразумевает, что будет 
заполнен весь буффер. Достаточно будет и 1 байта.
Т.е. не выйдем из функции пока что-то не придет.

Как серверные программы на блокирующем i/o завершают свою 
работу, если они ожидают на методе connect?
Просто делаем connect самому себе + выставляем какой-нибудь
флаг завершания.

Неблокирующий i/o реализуется той же самой функцией, но с
выставлением специального флага: fcntl(O_NONBLOCK);

В таком случае мы выйдем сразу, если не имеется никаких
данных.

Сигналы по своей природе являются асинхронным ipc.
Если мы можем спрогнозировать в какой момент времени какие данные мы будем
обрабатывать, то это синхронный механизм. Иначе - асинхронный.

Т.е. критерием асинхронности является неожиданность. Мы не можем знать
в какой момент времени начнется обработка данных.
(наш поток)								выполняет события из очереди
-----------------			инициируер работу		(может быть > 1 потока)
|   Initiator	|			-----------------		-------------------------
-----------------			| proactor	|------------->	| asynk operation	|
	|				-----------------		| demultiplexor		|
	|------------------------------------------------		-------------------------
	|			|			|			| (poll)
-----------------	-----------------	-----------------	-----------------
| completion	|	| async		|	| asynk opera-	|(fill)	|		|
| handler	|	| operation	|	| tion processor|-------| QUEUE EVENTS	|
| (callback)	|	| (r/w, etc...) |	| (fill queue	|	|		|
-----------------	-----------------	| of events	|	-----------------
						----------------- 	(складываются уже с данными)

Т.е. создаем операцию в нашем процессе, а обрабатывается событие в другом.
Можно создать операцию внутри обработчика события и тем самым сделать непрерывную 
рекурсивную обработку чего-либо.

Asynk operation processor работает поверх функции select (multiplexer)
в отдельном потоке. (select это блокирующая функция)

Аналогичные ф-ии селекторы:
	- select (O(n) сложность при обработку соединений от 
	числа дескрипторов)
	- poll
	- epoll (скорость работы не зависит от числа 
	дескрипторов)

Современные приложения, если OS поддерживает epoll, используют
именно его, т.к. данная функция намного производительней + 
существует несколько режимов работы, сильно схожих с 
асинхронным.

Для нашего процесса операция действительно выглядит асинхронной, но
на самом деле мы это скрыли в блокирующем вызове select в другом потоке.

В asynk operation processor мы реализуем так называемый паттерн
reactor, когда мы реагируем на какое либо внешнее событие.

Но не обязательно имитировать асинхронное вреимодействие через другой
поток + блокирующий вызов select. OS предоставляет специальный интерфейс
AIO (это часть POSIX-а). 

В той же манере позволяет заказать обработку некоторого события у OS.
Но в каком потоке будет исполняться обработка?
	- можно заказать обработку, через специальный обработчик сигналов
	- можно сделать в специальных потоках, чтобы они сделали что-то
	после получения события
Т.е. часть схемы может быть делигирована на уровень ниже в ядро OS.
Заказ какого-то события а не ожидание его говорит нам о том, что OS
может сделать некоторые вещи эффективней (См. /dev/epoll).

Виртуальное устройство, которое позволяет не копировать по нескольку 
раз буффер с данными.

Заказать можно лишь 1 асинхронную операцию на 1 сокет. 
После выполнения которой можно заказать ещё одну.
(т.к. мы не знаем порядок выполнения операции)

* Корутины (сопрограммы) - делают асинхронный по природе код, 
последовательно читаемым. (В С++20 вошли в стандарт как стандартное решение)

* Так же существует actor-ная модель взаимодействия. Когда декомпозируем такую
абстракцию как корутины на общение сообщениями между разными функциями.
- данная модель хорошо ложится для проектирования конечных автоматов.
Реализована как стандарт в {Scala, erlang}

class A {
	process()
	send()
}
методы могут исполняться на любых потоках
